{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is devoted to perform hyperparameter tunning of a DDQN agent in order to solve a pretty basic environment as Cartpole-v0. Besides learning how different hyperparameters affect the learning curve of our agent, this project is aimed at sanity checking the implementation of my agent, which will be used later on to solve more complex environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 09:54:53.788747: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-19 09:54:53.817672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-19 09:54:54.331091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils import create_settings, create_key, save_data\n",
    "import sys\n",
    "sys.path.insert(1,'/home/axelbm23/Code/ML_AI/Algos/ReinforcementLearning/')\n",
    "from agents import DDQN\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "from typing import Any,Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conduct some analysis on four different parameters as they seem seem to be the ones that affect the results in a major degree. For each parameter, we will produce 3 runs, as RL algorithms are more heavily influenced by the initial conditions (i.e network weights) that other techniques. The parameters we will play with are:\n",
    "1) complexity of the network, i.e number of layers and number of nodes per layer\n",
    "2) Learning rate of our optimizer\n",
    "3) batch size\n",
    "4) type of update on the target network, either soft update or hard copy.\n",
    "5) greedy step, i.e how large the exploratio phase is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axelbm23/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "2024-07-19 09:54:56.609041: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 09:54:56.670014: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 09:54:56.670049: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 09:54:56.685954: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 09:54:56.686009: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 09:54:56.686034: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 09:54:56.774349: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 09:54:56.774404: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 09:54:56.774410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-19 09:54:56.774434: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 09:54:56.774454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3542 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Set up the default values\n",
    "N_ITERATIONS = 3\n",
    "GAMMA = 0.99\n",
    "GREEDY_STEP = 999e-3\n",
    "EPISODES = 5\n",
    "BUFF_SIZE = 1_000\n",
    "BATCH_SIZE = 64\n",
    "NN_COPY_CADENCY = 10\n",
    "SOFT_UPDATE = 0.005\n",
    "NEURONS = [128]*2\n",
    "ACT_AS_IN = False\n",
    "ADD_LOGS = False\n",
    "LOSS_FUNC = 'mean_squared_error'\n",
    "ADAM_LR = 0.001\n",
    "OUTPUT_PATH = f'{os.getcwd()}/results/rewards_losses'\n",
    "\n",
    "# According to openai/gym/wiki\n",
    "# Cartpole-v0 is solved when it reaches an average reward\n",
    "# of 195 over 100 consecutive episodes\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "def_nn_arch = {'neurons': NEURONS,\n",
    "               'action_as_input':ACT_AS_IN,\n",
    "                'loss_function': LOSS_FUNC,\n",
    "                'optimizer': tf.keras.optimizers.Adam(learning_rate=ADAM_LR),\n",
    "                }\n",
    "\n",
    "def_agent = {'gamma': GAMMA,\n",
    "            'greedy_step': GREEDY_STEP,\n",
    "            'environment': env,\n",
    "            'episodes': EPISODES,\n",
    "            'buff_size': BUFF_SIZE, \n",
    "            'replay_mini_batch': BATCH_SIZE,\n",
    "            'nn_copy_cadency': NN_COPY_CADENCY,\n",
    "            'nn_architecture': def_nn_arch,\n",
    "            'soft_update': SOFT_UPDATE,\n",
    "            'add_logs':ADD_LOGS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different settings for each one of the experiments\n",
    "network_arch_sett = ([64]*2,[256]*4)\n",
    "lr_sett = [0.1,0.05]\n",
    "batch_siz_sett = [32,128]\n",
    "target_update_sett = [(25, 0.01),(None,0.005), (None,0.01)]\n",
    "greedy_step_sett = [99e-2, 9985e-4]\n",
    "experiment = {'nn_arch':network_arch_sett,\n",
    "               'lr':lr_sett,\n",
    "               'target_update':target_update_sett,\n",
    "               'greedy_step':greedy_step_sett,\n",
    "               'default':[None]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our agent for each set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard copy policy_weights to target_weights\n",
      "episode 0/4, greedy_param=0.97335 reward=27.0, avg_rew=27.0, avg_rew(100)=27.0\n",
      "episode 1/4, greedy_param=0.96077 reward=13.0, avg_rew=20.0, avg_rew(100)=20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721375697.568092  707139 service.cc:145] XLA service 0x7f975001e140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1721375697.568127  707139 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
      "2024-07-19 09:54:57.581627: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-19 09:54:57.628881: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1721375698.444056  707139 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2/4, greedy_param=0.92585 reward=37.0, avg_rew=25.667, avg_rew(100)=25.667\n",
      "episode 3/4, greedy_param=0.91389 reward=13.0, avg_rew=22.5, avg_rew(100)=22.5\n",
      "episode 4/4, greedy_param=0.89578 reward=20.0, avg_rew=22.0, avg_rew(100)=22.0\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/4, greedy_param=0.96462 reward=36.0, avg_rew=36.0, avg_rew(100)=36.0\n",
      "episode 1/4, greedy_param=0.95216 reward=13.0, avg_rew=24.5, avg_rew(100)=24.5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown variable: <KerasVariable shape=(4, 64), dtype=float32, path=sequential_2/dense_6/kernel>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m ddqn \u001b[38;5;241m=\u001b[39m DDQN(sett\u001b[38;5;241m=\u001b[39mag_sett)\n\u001b[1;32m      9\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 10\u001b[0m rewards, losses, logs \u001b[38;5;241m=\u001b[39m \u001b[43mddqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m exec_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt1, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save all the information we need for the post analysis,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# basically execution time, rewards and losses\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/ML_AI/Algos/ReinforcementLearning/agents.py:165\u001b[0m, in \u001b[0;36mDDQN.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soft_update_policy(ep)\n\u001b[1;32m    164\u001b[0m ep_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m--> 165\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_agent_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m s \u001b[38;5;241m=\u001b[39m s_prime\n\u001b[1;32m    167\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Code/ML_AI/Algos/ReinforcementLearning/agents.py:141\u001b[0m, in \u001b[0;36mDDQN._agent_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    140\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_regressors_targets()\n\u001b[0;32m--> 141\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:228\u001b[0m, in \u001b[0;36mBaseOptimizer._check_variables_are_known\u001b[0;34m(self, variables)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(v) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables_indices:\n\u001b[0;32m--> 228\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    229\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This optimizer can only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe called for the variables it was originally built with. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen working with a new set of variables, you should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecreate a new optimizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown variable: <KerasVariable shape=(4, 64), dtype=float32, path=sequential_2/dense_6/kernel>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
     ]
    }
   ],
   "source": [
    "for key,val in experiment.items():\n",
    "   for sett_i in val:\n",
    "      sett_key = create_key(sett_i)\n",
    "      for it in range(N_ITERATIONS):\n",
    "         ag_sett = create_settings(key, sett_i,def_nn_arch, def_agent)\n",
    "         model_label = f'{key}_{sett_key}_iter_{it}'if key!='default' else f'{key}_iter_{it}'\n",
    "         # Initialize the class again as the network weights need to be random\n",
    "         ddqn = DDQN(sett=ag_sett)\n",
    "         t1 = time.time()\n",
    "         rewards, losses, logs = ddqn.learn()\n",
    "         exec_time = round(time.time()-t1, 3)\n",
    "         \n",
    "         # Save all the information we need for the post analysis,\n",
    "         # basically execution time, rewards and losses\n",
    "         save_data(rewards, losses, exec_time, model_label, OUTPUT_PATH)\n",
    "         \n",
    "print(f'All data has been created')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
