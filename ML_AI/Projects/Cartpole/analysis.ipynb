{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is devoted to perform hyperparameter tunning of a DDQN agent in order to solve a pretty basic environment as Cartpole-v0. Besides learning how different hyperparameters affect the learning curve of our agent, this project is aimed at sanity checking the implementation of my agent, which will be used later on to solve more complex environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 14:19:24.912882: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-19 14:19:24.958674: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-19 14:19:25.482017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils import create_settings, create_key, save_data, solve_metric\n",
    "import sys\n",
    "sys.path.insert(1,'/home/axelbm23/Code/ML_AI/Algos/ReinforcementLearning/')\n",
    "from agents import DDQN,Agent_Performance\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "from typing import Any,Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conduct some analysis on four different parameters as they seem seem to be the ones that affect the results in a major degree. For each parameter, we will produce 3 runs, as RL algorithms are more heavily influenced by the initial conditions (i.e network weights) that other techniques. The parameters we will play with are:\n",
    "1) complexity of the network, i.e number of layers and number of nodes per layer\n",
    "2) Learning rate of our optimizer\n",
    "3) batch size\n",
    "4) type of update on the target network, either soft update or hard copy.\n",
    "5) greedy step, i.e how large the exploratio phase is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axelbm23/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "2024-07-19 14:19:27.622385: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 14:19:27.645105: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 14:19:27.645143: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 14:19:27.647208: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 14:19:27.647236: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 14:19:27.647249: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 14:19:27.722838: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 14:19:27.722900: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 14:19:27.722905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-19 14:19:27.722937: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 14:19:27.722956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3542 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Set up the default values\n",
    "N_ITERATIONS = 3\n",
    "GAMMA = 0.99\n",
    "GREEDY_STEP = 999e-3\n",
    "EPISODES = 3\n",
    "BUFF_SIZE = 1_000\n",
    "BATCH_SIZE = 64\n",
    "NN_COPY_CADENCY = 10\n",
    "SOFT_UPDATE = 0.005\n",
    "NEURONS = [128]*2\n",
    "ACT_AS_IN = False\n",
    "ADD_LOGS = False\n",
    "LOSS_FUNC = 'mean_squared_error'\n",
    "ADAM_LR = 0.001\n",
    "OUTPUT_PATH = f'{os.getcwd()}/results/rewards_losses'\n",
    "SOLVED = 195\n",
    "\n",
    "\n",
    "# According to openai/gym/wiki\n",
    "# Cartpole-v0 is solved when it reaches an average reward\n",
    "# of 195 over 100 consecutive episodes\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "def_nn_arch = {'neurons': NEURONS,\n",
    "               'action_as_input':ACT_AS_IN,\n",
    "                'loss_function': LOSS_FUNC,\n",
    "                'optimizer': tf.keras.optimizers.Adam(learning_rate=ADAM_LR),\n",
    "                }\n",
    "\n",
    "def_agent = {'gamma': GAMMA,\n",
    "            'greedy_step': GREEDY_STEP,\n",
    "            'environment': env,\n",
    "            'episodes': EPISODES,\n",
    "            'buff_size': BUFF_SIZE, \n",
    "            'replay_mini_batch': BATCH_SIZE,\n",
    "            'nn_copy_cadency': NN_COPY_CADENCY,\n",
    "            'nn_architecture': def_nn_arch,\n",
    "            'soft_update': SOFT_UPDATE,\n",
    "            'add_logs':ADD_LOGS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different settings for each one of the experiments\n",
    "network_arch_sett = ([64]*2,[256]*4)\n",
    "lr_sett = [0.1,0.05]\n",
    "batch_siz_sett = [32,128]\n",
    "target_update_sett = [(25, 0.01),(None,0.005), (None,0.01)]\n",
    "greedy_step_sett = [99e-2, 9985e-4]\n",
    "experiment = {'nn_arch':network_arch_sett,\n",
    "               'lr':lr_sett,\n",
    "               'target_update':target_update_sett,\n",
    "               'greedy_step':greedy_step_sett,\n",
    "               'default':[None]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our agent for each set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.97823 reward=22.0, avg_rew=22.0, avg_rew(100)=22.0\n",
      "episode 1/2, greedy_param=0.95216 reward=27.0, avg_rew=24.5, avg_rew(100)=24.5\n",
      "episode 2/2, greedy_param=0.93985 reward=13.0, avg_rew=20.667, avg_rew(100)=20.667\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.9753 reward=25.0, avg_rew=25.0, avg_rew(100)=25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721391568.599251  898376 service.cc:145] XLA service 0x7f53000076d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1721391568.599285  898376 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
      "2024-07-19 14:19:28.610822: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-19 14:19:28.654178: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1721391569.415404  898376 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1/2, greedy_param=0.93329 reward=44.0, avg_rew=34.5, avg_rew(100)=34.5\n",
      "episode 2/2, greedy_param=0.91389 reward=21.0, avg_rew=30.0, avg_rew(100)=30.0\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.9851 reward=15.0, avg_rew=15.0, avg_rew(100)=15.0\n",
      "episode 1/2, greedy_param=0.96559 reward=20.0, avg_rew=17.5, avg_rew(100)=17.5\n",
      "episode 2/2, greedy_param=0.94551 reward=21.0, avg_rew=18.667, avg_rew(100)=18.667\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.96946 reward=31.0, avg_rew=31.0, avg_rew(100)=31.0\n",
      "episode 1/2, greedy_param=0.95502 reward=15.0, avg_rew=23.0, avg_rew(100)=23.0\n",
      "episode 2/2, greedy_param=0.94079 reward=15.0, avg_rew=20.333, avg_rew(100)=20.333\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.97335 reward=27.0, avg_rew=27.0, avg_rew(100)=27.0\n",
      "episode 1/2, greedy_param=0.95885 reward=15.0, avg_rew=21.0, avg_rew(100)=21.0\n",
      "episode 2/2, greedy_param=0.94741 reward=12.0, avg_rew=18.0, avg_rew(100)=18.0\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.98314 reward=17.0, avg_rew=17.0, avg_rew(100)=17.0\n",
      "episode 1/2, greedy_param=0.96752 reward=16.0, avg_rew=16.5, avg_rew(100)=16.5\n",
      "episode 2/2, greedy_param=0.94741 reward=21.0, avg_rew=18.0, avg_rew(100)=18.0\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.96849 reward=32.0, avg_rew=32.0, avg_rew(100)=32.0\n",
      "episode 1/2, greedy_param=0.95406 reward=15.0, avg_rew=23.5, avg_rew(100)=23.5\n",
      "episode 2/2, greedy_param=0.91206 reward=45.0, avg_rew=30.667, avg_rew(100)=30.667\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.98807 reward=12.0, avg_rew=12.0, avg_rew(100)=12.0\n",
      "episode 1/2, greedy_param=0.96946 reward=19.0, avg_rew=15.5, avg_rew(100)=15.5\n",
      "episode 2/2, greedy_param=0.95216 reward=18.0, avg_rew=16.333, avg_rew(100)=16.333\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.9851 reward=15.0, avg_rew=15.0, avg_rew(100)=15.0\n",
      "episode 1/2, greedy_param=0.95025 reward=36.0, avg_rew=25.5, avg_rew(100)=25.5\n",
      "episode 2/2, greedy_param=0.92493 reward=27.0, avg_rew=26.0, avg_rew(100)=26.0\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.98609 reward=14.0, avg_rew=14.0, avg_rew(100)=14.0\n",
      "episode 1/2, greedy_param=0.96559 reward=21.0, avg_rew=17.5, avg_rew(100)=17.5\n",
      "episode 2/2, greedy_param=0.92123 reward=47.0, avg_rew=27.333, avg_rew(100)=27.333\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.99004 reward=10.0, avg_rew=10.0, avg_rew(100)=10.0\n",
      "episode 1/2, greedy_param=0.96752 reward=23.0, avg_rew=16.5, avg_rew(100)=16.5\n",
      "episode 2/2, greedy_param=0.94551 reward=23.0, avg_rew=18.667, avg_rew(100)=18.667\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.98609 reward=14.0, avg_rew=14.0, avg_rew(100)=14.0\n",
      "episode 1/2, greedy_param=0.97335 reward=13.0, avg_rew=13.5, avg_rew(100)=13.5\n",
      "episode 2/2, greedy_param=0.95598 reward=18.0, avg_rew=15.0, avg_rew(100)=15.0\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.99104 reward=9.0, avg_rew=9.0, avg_rew(100)=9.0\n",
      "episode 1/2, greedy_param=0.97823 reward=13.0, avg_rew=11.0, avg_rew(100)=11.0\n",
      "episode 2/2, greedy_param=0.96462 reward=14.0, avg_rew=12.0, avg_rew(100)=12.0\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.95025 reward=51.0, avg_rew=51.0, avg_rew(100)=51.0\n",
      "episode 1/2, greedy_param=0.93329 reward=18.0, avg_rew=34.5, avg_rew(100)=34.5\n",
      "episode 2/2, greedy_param=0.92031 reward=14.0, avg_rew=27.667, avg_rew(100)=27.667\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.99104 reward=9.0, avg_rew=9.0, avg_rew(100)=9.0\n",
      "episode 1/2, greedy_param=0.95693 reward=35.0, avg_rew=22.0, avg_rew(100)=22.0\n",
      "episode 2/2, greedy_param=0.94646 reward=11.0, avg_rew=18.333, avg_rew(100)=18.333\n",
      "Hard copy policy_weights to target_weights\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 0/2, greedy_param=0.9851 reward=15.0, avg_rew=15.0, avg_rew(100)=15.0\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 1/2, greedy_param=0.97335 reward=12.0, avg_rew=13.5, avg_rew(100)=13.5\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 2/2, greedy_param=0.95789 reward=16.0, avg_rew=14.333, avg_rew(100)=14.333\n",
      "Hard copy policy_weights to target_weights\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 0/2, greedy_param=0.98314 reward=17.0, avg_rew=17.0, avg_rew(100)=17.0\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 1/2, greedy_param=0.9714 reward=12.0, avg_rew=14.5, avg_rew(100)=14.5\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 2/2, greedy_param=0.96269 reward=9.0, avg_rew=12.667, avg_rew(100)=12.667\n",
      "Hard copy policy_weights to target_weights\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 0/2, greedy_param=0.98117 reward=19.0, avg_rew=19.0, avg_rew(100)=19.0\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 1/2, greedy_param=0.96462 reward=17.0, avg_rew=18.0, avg_rew(100)=18.0\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 2/2, greedy_param=0.94741 reward=18.0, avg_rew=18.0, avg_rew(100)=18.0\n",
      "Hard copy policy_weights to target_weights\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 0/2, greedy_param=0.96559 reward=35.0, avg_rew=35.0, avg_rew(100)=35.0\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 1/2, greedy_param=0.94268 reward=24.0, avg_rew=29.5, avg_rew(100)=29.5\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 2/2, greedy_param=0.89578 reward=51.0, avg_rew=36.667, avg_rew(100)=36.667\n",
      "Hard copy policy_weights to target_weights\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 0/2, greedy_param=0.98412 reward=16.0, avg_rew=16.0, avg_rew(100)=16.0\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 1/2, greedy_param=0.97237 reward=12.0, avg_rew=14.0, avg_rew(100)=14.0\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 2/2, greedy_param=0.95311 reward=20.0, avg_rew=16.0, avg_rew(100)=16.0\n",
      "Hard copy policy_weights to target_weights\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 0/2, greedy_param=0.9851 reward=15.0, avg_rew=15.0, avg_rew(100)=15.0\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 1/2, greedy_param=0.93704 reward=50.0, avg_rew=32.5, avg_rew(100)=32.5\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "Copying weights using soft_update\n",
      "episode 2/2, greedy_param=0.92216 reward=16.0, avg_rew=27.0, avg_rew(100)=27.0\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.81791 reward=20.0, avg_rew=20.0, avg_rew(100)=20.0\n",
      "episode 1/2, greedy_param=0.7323 reward=11.0, avg_rew=15.5, avg_rew(100)=15.5\n",
      "episode 2/2, greedy_param=0.66228 reward=10.0, avg_rew=13.667, avg_rew(100)=13.667\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.74717 reward=29.0, avg_rew=29.0, avg_rew(100)=29.0\n",
      "episode 1/2, greedy_param=0.53091 reward=34.0, avg_rew=31.5, avg_rew(100)=31.5\n",
      "episode 2/2, greedy_param=0.42133 reward=23.0, avg_rew=28.667, avg_rew(100)=28.667\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.92274 reward=8.0, avg_rew=8.0, avg_rew(100)=8.0\n",
      "episode 1/2, greedy_param=0.75472 reward=20.0, avg_rew=14.0, avg_rew(100)=14.0\n",
      "episode 2/2, greedy_param=0.68255 reward=10.0, avg_rew=12.667, avg_rew(100)=12.667\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.95453 reward=31.0, avg_rew=31.0, avg_rew(100)=31.0\n",
      "episode 1/2, greedy_param=0.91937 reward=25.0, avg_rew=28.0, avg_rew(100)=28.0\n",
      "episode 2/2, greedy_param=0.90296 reward=12.0, avg_rew=22.667, avg_rew(100)=22.667\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.90704 reward=65.0, avg_rew=65.0, avg_rew(100)=65.0\n",
      "episode 1/2, greedy_param=0.89218 reward=11.0, avg_rew=38.0, avg_rew(100)=38.0\n",
      "episode 2/2, greedy_param=0.87232 reward=15.0, avg_rew=30.333, avg_rew(100)=30.333\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.98067 reward=13.0, avg_rew=13.0, avg_rew(100)=13.0\n",
      "episode 1/2, greedy_param=0.89084 reward=64.0, avg_rew=38.5, avg_rew(100)=38.5\n",
      "episode 2/2, greedy_param=0.87494 reward=12.0, avg_rew=29.667, avg_rew(100)=29.667\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.98708 reward=13.0, avg_rew=13.0, avg_rew(100)=13.0\n",
      "episode 1/2, greedy_param=0.9753 reward=12.0, avg_rew=12.5, avg_rew(100)=12.5\n",
      "episode 2/2, greedy_param=0.95502 reward=21.0, avg_rew=15.333, avg_rew(100)=15.333\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.98609 reward=14.0, avg_rew=14.0, avg_rew(100)=14.0\n",
      "episode 1/2, greedy_param=0.97335 reward=13.0, avg_rew=13.5, avg_rew(100)=13.5\n",
      "episode 2/2, greedy_param=0.93797 reward=37.0, avg_rew=21.333, avg_rew(100)=21.333\n",
      "Hard copy policy_weights to target_weights\n",
      "episode 0/2, greedy_param=0.99203 reward=8.0, avg_rew=8.0, avg_rew(100)=8.0\n",
      "episode 1/2, greedy_param=0.96752 reward=25.0, avg_rew=16.5, avg_rew(100)=16.5\n",
      "episode 2/2, greedy_param=0.95121 reward=17.0, avg_rew=16.667, avg_rew(100)=16.667\n",
      "All data has been created\n"
     ]
    }
   ],
   "source": [
    "for key,val in experiment.items():\n",
    "   for sett_i in val:\n",
    "      sett_key = create_key(sett_i)\n",
    "      for it in range(N_ITERATIONS):\n",
    "         ag_sett = create_settings(key,sett_i,def_nn_arch, def_agent)\n",
    "         model_label = f'{key}_{sett_key}_iter_{it}'if key!='default' else f'{key}_iter_{it}'\n",
    "         # Initialize the class again as the network weights need to be random\n",
    "         ddqn = DDQN(sett=ag_sett)\n",
    "         t1 = time.time()\n",
    "         rewards, losses, logs = ddqn.learn()\n",
    "         exec_time = round(time.time()-t1, 3)\n",
    "         \n",
    "         # Save all the information we need for the post analysis,\n",
    "         # basically execution time, rewards and losses\n",
    "         save_data(rewards, losses, exec_time, model_label, OUTPUT_PATH)\n",
    "         \n",
    "print(f'All data has been created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform two tasks here. The first one will be to analyze the reward/loss function of each individual try of the algorithm to make some hypothesis about what is going on with it. The second part, will be to compute some performance metrics to quantify the algorithm performance across different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the saved files and compute statistics for each set of params\n",
    "files = glob.glob(f'{OUTPUT_PATH}/*.csv')\n",
    "param_sett_ids = set(x.split('/')[-1].split('_iter')[0] for x in files)\n",
    "stats = {}\n",
    "for param in param_sett_ids:\n",
    "    # Select only the trials of this parameter settings\n",
    "    files_for_this_param = [x for x in files if x.split('/')[-1].split('_iter')[0]==param]\n",
    "    rew = []\n",
    "    loss = []\n",
    "    for trial_param in files_for_this_param:\n",
    "        df = pd.read_csv(trial_param)\n",
    "        rew.append(df['rewards'].to_numpy())\n",
    "        loss.append(df['losses'].to_numpy())\n",
    "    ag_perf = Agent_Performance(rewards=rew, losses=loss, solved_func=solve_metric, solved_rew=SOLVED)\n",
    "    stats[param] = ag_perf.compute_statistics()\n",
    "# Create a dataframe where each row is the param settings and each column is the statistic \n",
    "stats_df = pd.DataFrame.from_dict(stats).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_update_25-0.01</th>\n",
       "      <th>target_update_None-0.01</th>\n",
       "      <th>greedy_step_0.99</th>\n",
       "      <th>nn_arch_256-256-256-256</th>\n",
       "      <th>lr_0.1</th>\n",
       "      <th>greedy_step_0.9985</th>\n",
       "      <th>default</th>\n",
       "      <th>target_update_None-0.005</th>\n",
       "      <th>nn_arch_64-64</th>\n",
       "      <th>lr_0.05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_end_rew</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>18.333333</td>\n",
       "      <td>29.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_worst_ratio_end_rew</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>2.187500</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.176471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_epi_rew</th>\n",
       "      <td>19.333333</td>\n",
       "      <td>26.555556</td>\n",
       "      <td>18.333333</td>\n",
       "      <td>18.777778</td>\n",
       "      <td>24.333333</td>\n",
       "      <td>27.555556</td>\n",
       "      <td>17.777778</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>23.111111</td>\n",
       "      <td>20.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_epi_rew_increase</th>\n",
       "      <td>-5.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>-2.333333</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>-11.666667</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>-1.166667</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_epi_rew_increase</th>\n",
       "      <td>16.708281</td>\n",
       "      <td>23.001812</td>\n",
       "      <td>8.185353</td>\n",
       "      <td>5.575243</td>\n",
       "      <td>16.260894</td>\n",
       "      <td>34.183329</td>\n",
       "      <td>10.606602</td>\n",
       "      <td>2.273030</td>\n",
       "      <td>13.357270</td>\n",
       "      <td>6.867799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_solved</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_ep_to_solve</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_drawdown</th>\n",
       "      <td>20.333333</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          target_update_25-0.01  target_update_None-0.01  \\\n",
       "avg_end_rew                           13.000000                29.000000   \n",
       "best_worst_ratio_end_rew               0.272727                 2.187500   \n",
       "avg_epi_rew                           19.333333                26.555556   \n",
       "avg_epi_rew_increase                  -5.000000                 3.500000   \n",
       "std_epi_rew_increase                  16.708281                23.001812   \n",
       "n_solved                               0.000000                 0.000000   \n",
       "avg_ep_to_solve                             NaN                      NaN   \n",
       "max_drawdown                          20.333333                11.333333   \n",
       "\n",
       "                          greedy_step_0.99  nn_arch_256-256-256-256  \\\n",
       "avg_end_rew                      14.333333                16.000000   \n",
       "best_worst_ratio_end_rew          1.300000                 0.750000   \n",
       "avg_epi_rew                      18.333333                18.777778   \n",
       "avg_epi_rew_increase             -2.333333                -4.500000   \n",
       "std_epi_rew_increase              8.185353                 5.575243   \n",
       "n_solved                          0.000000                 0.000000   \n",
       "avg_ep_to_solve                        NaN                      NaN   \n",
       "max_drawdown                     10.333333                10.333333   \n",
       "\n",
       "                             lr_0.1  greedy_step_0.9985    default  \\\n",
       "avg_end_rew               30.000000           13.000000  25.000000   \n",
       "best_worst_ratio_end_rew   1.500000            0.250000   1.176471   \n",
       "avg_epi_rew               24.333333           27.555556  17.777778   \n",
       "avg_epi_rew_increase       5.166667          -11.666667   6.666667   \n",
       "std_epi_rew_increase      16.260894           34.183329  10.606602   \n",
       "n_solved                   0.000000            0.000000   0.000000   \n",
       "avg_ep_to_solve                 NaN                 NaN        NaN   \n",
       "max_drawdown               3.333333           41.666667   2.666667   \n",
       "\n",
       "                          target_update_None-0.005  nn_arch_64-64    lr_0.05  \n",
       "avg_end_rew                              14.333333      18.333333  29.333333  \n",
       "best_worst_ratio_end_rew                  1.000000       0.615385   1.611111  \n",
       "avg_epi_rew                              15.000000      23.111111  20.333333  \n",
       "avg_epi_rew_increase                     -1.333333      -1.166667   8.333333  \n",
       "std_epi_rew_increase                      2.273030      13.357270   6.867799  \n",
       "n_solved                                  0.000000       0.000000   0.000000  \n",
       "avg_ep_to_solve                                NaN            NaN        NaN  \n",
       "max_drawdown                              3.333333      12.333333   0.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
